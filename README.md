# Multimodal Foundation Models

> Usage: This repository is used to our paper on Multimodal Foundation Models.<br>
> Target: We are collecting some papers on it for our review, so the target is going.

## Content

- [1. Review](#1)
  - [1.1 AGI (Artificial general intelligence)](#1.1)
    - [1.1.1 Sparks of Artificial General Intelligence: Early experiments with GPT-4](#1.1.1)
  - [1.2 Multimodal Foundation Models](#1.2)
    - [1.2.1 Segment Anything](#1.2.1)
    - [1.2.2 GPT-4 Technical Report](#1.2.2)
    - [1.2.3 Language Is Not All You Need: Aligning Perception with Language Models](#1.2.3)
  - [1.3 Prompt](#1.3)
    - [1.3.1 Images Speak in Images: A Generalist Painter for In-Context Visual Learning](#1.3.1)
    - [1.3.2 SegGPT: Segmenting Everything In Context](#1.3.2)
  - [1.4 Adapter](#1.4)
    - [1.4.1 LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention](#1.4.1)
    - [1.4.2 SegGPT: Segmenting Everything In Context](#1.4.2)
  - [1.5 CLIP\text-image alignment](#1.5)
    - [1.5.1 LiT: Zero-Shot Transfer with Locked-image text Tuning](#1.5.1)
    - [1.5.2 Learning Transferable Visual Models From Natural Language Supervision](#1.5.2)
    - [1.5.3 ClipCap: CLIP Prefix for Image Captioning](#1.5.3)
    - [1.5.4 BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](#1.5.4)
    - [1.5.5 BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](#1.5.5)
  - [1.6 3D Field ](#1.6)
    - [1.6.1 Anything-3D: Towards Single-view Anything Reconstruction in the Wild](#1.6.1)

# Review<a id="1"></a>

## AGI (Artificial general intelligence)<a id="1.1"></a>


* Sparks of Artificial General Intelligence: Early experiments with GPT-4 [[view]](https://arxiv.org/abs/2303.12712)<a id="1.1.1"></a>

## Multimodal Foundation Models<a id="1.2"></a>

* Segment Anything [[view]](https://arxiv.org/abs/2304.02643)<a id="1.2.1"></a>

* GPT-4 Technical Report [[view]](https://arxiv.org/abs/2303.08774)<a id="1.2.2"></a>

* Language Is Not All You Need: Aligning Perception with Language Models [[view]](https://arxiv.org/abs/2302.14045)<a id="1.2.3"></a>

## Prompt<a id="1.3"></a>

* Images Speak in Images: A Generalist Painter for In-Context Visual Learning [[view]](https://arxiv.org/abs/2212.02499)<a id="1.3.1"></a>

* SegGPT: Segmenting Everything In Context [[view]](https://arxiv.org/abs/2304.03284)<a id="1.3.2"></a>

## Adapter<a id="1.4"></a>

* LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention [[view]](https://arxiv.org/abs/2303.16199)<a id="1.4.1"></a>

## CLIP\text-image alignment<a id="1.5"></a>

* LiT: Zero-Shot Transfer with Locked-image text Tuning [[view]](https://arxiv.org/abs/2111.07991v3)<a id="1.5.1"></a>

* Learning Transferable Visual Models From Natural Language Supervision [[view]](https://arxiv.org/abs/2103.00020)<a id="1.5.2"></a>

* ClipCap: CLIP Prefix for Image Captioning [[view]](https://arxiv.org/abs/2111.09734)<a id="1.5.3"></a>

* BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation [[view]](https://arxiv.org/abs/2201.12086)<a id="1.5.4"></a>

* BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models [[view]](https://arxiv.org/abs/2301.12597)<a id="1.5.5"></a>

## 3D<a id="1.6"></a>

* Anything-3D: Towards Single-view Anything Reconstruction in the Wild [[view]](https://arxiv.org/abs/2304.10261)<a id="1.6.1"></a>

### More papers or survey, to be continued...
