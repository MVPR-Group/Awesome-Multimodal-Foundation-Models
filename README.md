# Review on Multimodal Foundation Models

Papers on multimodal foundation models

## AGI

Sparks of Artificial General Intelligence: Early experiments with GPT-4 [paper](https://arxiv.org/abs/2303.12712)

## Multimodal Foundation Models

Segment Anything [paper](https://arxiv.org/abs/2304.02643)

GPT-4 Technical Report [paper](https://arxiv.org/abs/2303.08774)

Language Is Not All You Need: Aligning Perception with Language Models [paper](https://arxiv.org/abs/2302.14045)

## Prompt

Images Speak in Images: A Generalist Painter for In-Context Visual Learning [paper](https://arxiv.org/abs/2212.02499)

SegGPT: Segmenting Everything In Context [paper](https://arxiv.org/abs/2304.03284)

## Adapter

LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention [paper](https://arxiv.org/abs/2303.16199)

## CLIP\text-image alignment

LiT: Zero-Shot Transfer with Locked-image text Tuning [paper](https://arxiv.org/abs/2111.07991v3)

Learning Transferable Visual Models From Natural Language Supervision [paper](https://arxiv.org/abs/2103.00020)

ClipCap: CLIP Prefix for Image Captioning [paper](https://arxiv.org/abs/2111.09734)

BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation [paper](https://arxiv.org/abs/2201.12086)

BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models [paper](https://arxiv.org/abs/2301.12597)

## 3D

Anything-3D: Towards Single-view Anything Reconstruction in the Wild [paper](https://arxiv.org/abs/2304.10261)

To be countinued...
