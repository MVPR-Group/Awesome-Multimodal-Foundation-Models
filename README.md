# Multimodal Foundation Models

> Usage: This repository is used to our paper on Multimodal Foundation Models.<br>
> Target: We are collecting some papers on it for our review, so the target is going.

## Content

- [1. Review](#1)
  - [1.1 AGI (Artificial general intelligence)](#1.1)
    - [1.1.1 Sparks of Artificial General Intelligence: Early experiments with GPT-4](#1.1.1)
  - [1.2 Multimodal Foundation Models](#1.2)
    - [1.2.1 Segment Anything](#1.2.1)
    - [1.2.2 GPT-4 Technical Report](#1.2.2)
    - [1.2.3 Language Is Not All You Need: Aligning Perception with Language Models](#1.2.3)
  - [1.3 Prompt](#1.3)
    - [1.3.1 Images Speak in Images: A Generalist Painter for In-Context Visual Learning](#1.3.1)
    - [1.3.2 SegGPT: Segmenting Everything In Context](#1.3.2)
  - [1.4 Adapter](#1.4)
    - [1.4.1 LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention](#1.4.1)
    - [1.4.2 SegGPT: Segmenting Everything In Context](#1.4.2)
  - [1.5 CLIP\text-image alignment](#1.5)
    - [1.5.1 LiT: Zero-Shot Transfer with Locked-image text Tuning](#1.5.1)
    - [1.5.2 Learning Transferable Visual Models From Natural Language Supervision](#1.5.2)
    - [1.5.3 ClipCap: CLIP Prefix for Image Captioning](#1.5.3)
    - [1.5.4 BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](#1.5.4)
    - [1.5.5 BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](#1.5.5)
  - [1.6 3D Field ](#1.6)
    - [1.6.1 Anything-3D: Towards Single-view Anything Reconstruction in the Wild](#1.6.1)
    - [1.6.2 Point-E- A System for Generating3D Point Clouds from Complex](#1.6.2)
    - [1.6.3 Shap-E: Generating Conditional 3D Implicit Functions](#1.6.3)
  - [1.7 Autonomous Driving ](#1.7)
    - [1.7.1 CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP](#1.7.1)
    - [1.7.2 UniAD：Planning-oriented Autonomous Driving](#1.7.2)
    - [1.7.3 ST-P3: End-to-end Vision-based Autonomous Driving via Spatial-Temporal Feature Learning](#1.7.3)
    - [1.7.4 ViP3D: End-to-end Visual Trajectory Prediction via 3D Agent Queries](#1.7.4)
  - [1.8 Diffusion](#1.8)
    - [1.8.1 DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models](#1.8.1)
    - [1.8.2 DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation](#1.8.2)
    - [1.8.3 Scalable Diffusion Models with Transformers](#1.8.3)

# Review<a id="1"></a>

## AGI (Artificial general intelligence)<a id="1.1"></a>


* Sparks of Artificial General Intelligence: Early experiments with GPT-4 [[view]](https://arxiv.org/abs/2303.12712)<a id="1.1.1"></a>

## Multimodal Foundation Models<a id="1.2"></a>

* Segment Anything [[view]](https://arxiv.org/abs/2304.02643)<a id="1.2.1"></a>

* GPT-4 Technical Report [[view]](https://arxiv.org/abs/2303.08774)<a id="1.2.2"></a>

* Language Is Not All You Need: Aligning Perception with Language Models [[view]](https://arxiv.org/abs/2302.14045)<a id="1.2.3"></a>

## Prompt<a id="1.3"></a>

* Images Speak in Images: A Generalist Painter for In-Context Visual Learning [[view]](https://arxiv.org/abs/2212.02499)<a id="1.3.1"></a>

* SegGPT: Segmenting Everything In Context [[view]](https://arxiv.org/abs/2304.03284)<a id="1.3.2"></a>

## Adapter<a id="1.4"></a>

* LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention [[view]](https://arxiv.org/abs/2303.16199)<a id="1.4.1"></a>

## CLIP\text-image alignment<a id="1.5"></a>

* LiT: Zero-Shot Transfer with Locked-image text Tuning [[view]](https://arxiv.org/abs/2111.07991v3)<a id="1.5.1"></a>

* Learning Transferable Visual Models From Natural Language Supervision [[view]](https://arxiv.org/abs/2103.00020)<a id="1.5.2"></a>

* ClipCap: CLIP Prefix for Image Captioning [[view]](https://arxiv.org/abs/2111.09734)<a id="1.5.3"></a>

* BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation [[view]](https://arxiv.org/abs/2201.12086)<a id="1.5.4"></a>

* BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models [[view]](https://arxiv.org/abs/2301.12597)<a id="1.5.5"></a>

## 3D<a id="1.6"></a>

* Anything-3D: Towards Single-view Anything Reconstruction in the Wild [[view]](https://arxiv.org/abs/2304.10261)<a id="1.6.1"></a>
* Point-E- A System for Generating3D Point Clouds from Complex [[view]](https://arxiv.org/abs/2212.08751)<a id="1.6.1"></a>
* Shap-E: Generating Conditional 3D Implicit Functions [[view]](https://arxiv.org/abs/2305.02463)<a id="1.6.1"></a>

## Autonomous Driving<a id="1.7"></a>

* CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP [[view]](https://arxiv.org/pdf/2301.04926.pdf)<a id="1.7.1"></a>
* UniAD：Planning-oriented Autonomous Driving [[view]](https://arxiv.org/abs/2212.10156)<a id="1.7.2"></a>
* ST-P3: End-to-end Vision-based Autonomous Driving via Spatial-Temporal Feature Learning [[view]](https://arxiv.org/pdf/2207.07601.pdf)<a id="1.7.3"></a>
* ViP3D: End-to-end Visual Trajectory Prediction via 3D Agent Queries [[view]](https://arxiv.org/abs/2208.01582)<a id="1.7.4"></a>

## Diffusion<a id="1.8"></a>

* DiffusionBERT: Improving Generative Masked Language Models with Diffusion Models [[view]](https://arxiv.org/abs/2211.15029)<a id="1.8.1"></a>
* DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation [[view]](https://arxiv.org/abs/2110.02711)<a id="1.8.2"></a>
* Scalable Diffusion Models with Transformers [[view]](https://arxiv.org/abs/2212.09748)<a id="1.8.3"></a>

### More papers or survey, to be continued...
